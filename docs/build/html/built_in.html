
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="Python">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Core Built-in Functions &#8212; BOML 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Extensible Modules" href="extension.html" />
    <link rel="prev" title="Core Modules" href="modules.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="core-built-in-functions">
<h1>Core Built-in Functions<a class="headerlink" href="#core-built-in-functions" title="Permalink to this headline">Â¶</a></h1>
<ol class="arabic">
<li><p class="first">BOMLOptimizer.meta_learner:</p>
<ul>
<li><dl class="first docutils">
<dt>Aliases:</dt>
<dd><blockquote class="first">
<div><ul class="simple">
<li>boml.boml_optimizer.BOMLOptimizer.meta_learner()</li>
</ul>
</div></blockquote>
<div class="last highlight-sh"><div class="highlight"><pre><span></span>boml.boml_optimizer.BOMLOptimizer.meta_learner<span class="o">(</span>
        _input,
        dataset,
        <span class="nv">meta_model</span><span class="o">=</span><span class="s1">&#39;V1&#39;</span>,
        <span class="nv">name</span><span class="o">=</span><span class="s1">&#39;Hyper_Net&#39;</span>,
        <span class="nv">use_T</span><span class="o">=</span>False,
        <span class="nv">use_Warp</span><span class="o">=</span>False,
        **model_args
<span class="o">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
<p>This method must be called once at first to build meta modules and initialize meta parameters and neural networks.</p>
<ul>
<li><p class="first">Args:</p>
<blockquote>
<div><ul class="simple">
<li>_input: orginal input for neural network construction;</li>
<li>dataset: which dataset to use for training and testing. It should be initialized before being passed into the function</li>
<li>meta_model: model chosen for neural network construction, <cite>V1</cite> for C4L with fully connected layer,`V2` for Residual blocks with fully connected layer.</li>
<li>name: name for Meta model modules used for BOMLNet initialization</li>
<li>use_T: whether to use T layer for C4L neural networks</li>
<li>use_Warp: whether to use Warp layer for C4L neural networks</li>
<li>model_args: optional arguments to set specific parameters of neural networks.</li>
</ul>
</div></blockquote>
</li>
</ul>
</li>
<li><p class="first">BOMLOptimizer.base_learner:</p>
<ul>
<li><dl class="first docutils">
<dt>Aliases:</dt>
<dd><blockquote class="first">
<div><ul class="simple">
<li>boml.boml_optimizer.BOMLOptimizer.base_learner()</li>
</ul>
</div></blockquote>
<div class="last highlight-sh"><div class="highlight"><pre><span></span>boml.boml_optimizer.BOMLOptimizer.base_learner<span class="o">(</span>
        _input,
        meta_learner, <span class="nv">name</span><span class="o">=</span><span class="s1">&#39;Task_Net&#39;</span>,
        <span class="nv">weights_initializer</span><span class="o">=</span>tf.zeros_initializer
<span class="o">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
</ul>
<p>This method has to be called for every experiment and takes responsibility for defining task-specific modules and inner optimizer.</p>
<ul>
<li><p class="first">Args:</p>
<blockquote>
<div><ul class="simple">
<li>_input: orginal input for neural network construction of task-specific module;</li>
<li>meta_learner: returned value of meta_learner function, which is a instance of BOMLNet or its child classes</li>
<li>name: name for Base model modules used for BOMLNet initialization</li>
<li>weights_initializer: initializer function for task_specific network, called by 'MetaRepr' method</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Returns: task-specific model part</p>
</li>
</ul>
</li>
<li><p class="first">BOMLOptimizer.ll_problem:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt>Aliases:</dt>
<dd><ul class="first last">
<li>boml.boml_optimizer.BOMLOptimizer.ll_problem()</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="highlight-sh"><div class="highlight"><pre><span></span>boml.boml_optimizer.BOMLOptimizer.ll_problem<span class="o">(</span>
          inner_objective,
          learning_rate,
          T,
          <span class="nv">inner_objective_optimizer</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span>,
          <span class="nv">outer_objective</span><span class="o">=</span>None,
          <span class="nv">learn_lr</span><span class="o">=</span>False,
          <span class="nv">alpha_init</span><span class="o">=</span><span class="m">0</span>.0,
          <span class="nv">s</span><span class="o">=</span><span class="m">1</span>.0, <span class="nv">t</span><span class="o">=</span><span class="m">1</span>.0,
          <span class="nv">learn_alpha</span><span class="o">=</span>False,
          <span class="nv">learn_st</span><span class="o">=</span>False,
          <span class="nv">learn_alpha_itr</span><span class="o">=</span>False,
          <span class="nv">var_list</span><span class="o">=</span>None,
          <span class="nv">init_dynamics_dict</span><span class="o">=</span>None,
          <span class="nv">first_order</span><span class="o">=</span>False,
          <span class="nv">loss_func</span><span class="o">=</span>utils.cross_entropy,
          <span class="nv">momentum</span><span class="o">=</span><span class="m">0</span>.5,
          <span class="nv">beta1</span><span class="o">=</span><span class="m">0</span>.0,
          <span class="nv">beta2</span><span class="o">=</span><span class="m">0</span>.999,
          <span class="nv">regularization</span><span class="o">=</span>None,
          <span class="nv">experiment</span><span class="o">=</span>None,
          <span class="nv">scalor</span><span class="o">=</span><span class="m">0</span>.0,
          **inner_kargs
<span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
<p>After construction of neural networks, solutions to lower level problems should be regulated in ll_problem.</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>Args:</dt>
<dd><ul class="first last">
<li>inner_objective: loss function for the inner optimization problem</li>
<li>learning_rate: step size for inner loop optimization</li>
<li>T: numbers of steps for inner gradient descent optimization</li>
<li>inner_objective_optimizer: Optimizer type for the outer parameters, should be in list [<cite>SGD</cite>,`Momentum`,`Adam`]</li>
<li>outer_objective: loss function for the outer optimization problem, which need to be claimed in BDA agorithm</li>
<li>alpha_init: initial value of ratio of inner objective to outer objective in BDA algorithm</li>
<li>s,t: coefficients of aggregation of inner and outer objectives in BDA algorithm, default to be 1.0</li>
<li>learn_alpha: specify parameter for BDA algorithm to decide whether to initialize alpha as a hyper parameter</li>
<li>learn_alpha_itr: parameter for BDA algorithm to specify whether to initialize alpha as a vector, of which every dimension's value is step-wise scale factor fot the optimization process</li>
<li>learn_st: specify parameter for BDA algorithm to decide whether to initialize s and t as hyper parameters</li>
<li>first_order: specific parameter to define whether to use implement first order MAML, default to be <cite>FALSE</cite></li>
<li>loss_func: specifying which type of loss function is used for the maml-based method, which should be consistent with the form to compute the inner objective</li>
<li>momentum: specific parameter for Optimizer.BOMLOptMomentum to set initial value of momentum</li>
<li>regularization: whether to add regularization terms in the inner objective</li>
<li>experiment: instance of Experiment to use in the Lower Level Problem, especifially needed in the <cite>MetaRper</cite> type of method.</li>
<li>var_list: optional list of variables (of the inner optimization problem)</li>
<li>inner_kargs: optional arguments to pass to <cite>boml.boml_optimizer.BOMLOptimizer.compute_gradients</cite></li>
</ul>
</dd>
</dl>
</li>
<li>Returns: task-specific model part</li>
</ul>
</li>
<li><p class="first">BOMLOptimizer.ul_problem</p>
<ul>
<li><p class="first">Aliases:</p>
<blockquote>
<div><blockquote>
<div><ul>
<li><p class="first">boml.boml_optimizer.BOMLOptimizer.ul_problem()</p>
<blockquote>
<div><div class="highlight-sh"><div class="highlight"><pre><span></span>boml.boml_optimizer.BOMLOptimizer.ul_Problem<span class="o">(</span>
        outer_objective,
        meta_learning_rate,
        inner_grad,
        <span class="nv">meta_param</span><span class="o">=</span>None,
        <span class="nv">outer_objective_optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span>,
        <span class="nv">epsilon</span><span class="o">=</span><span class="m">1</span>.0,
        <span class="nv">momentum</span><span class="o">=</span><span class="m">0</span>.5,
        <span class="nv">global_step</span><span class="o">=</span>None
<span class="o">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>This method define upper level problems and choose optimizer to optimize meta parameters, which should be called afer ll_problem.</p>
<ul>
<li><p class="first">Args:</p>
<blockquote>
<div><ul class="simple">
<li>outer_objective: scalar tensor for the outer objective</li>
<li>meta_learning_rate: step size for outer loop optimization</li>
<li>inner_grad: Returned value of boml.BOMLOptimizer.LLProblem()</li>
<li>meta_param: optional list of outer parameters and model parameters</li>
<li>outer_objective_optimizer: Optimizer type for the outer parameters, should be in list [<cite>SGD</cite>,`Momentum`,`Adam`]</li>
<li>epsilon: Float, cofffecients to be used in DARTS algorithm</li>
<li>momentum: specific parameters to be used to initialize <cite>Momentum</cite> algorithm</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Returnsï¼meta_param list, used for debugging</p>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</li>
<li><p class="first">aggregate_all:</p>
<ul>
<li><p class="first">Aliases:</p>
<blockquote>
<div><blockquote>
<div><ul class="simple">
<li>boml.boml_optimizer.BOMLOptimizer.aggregate_all()</li>
</ul>
</div></blockquote>
<dl class="docutils">
<dt>::</dt>
<dd><dl class="first last docutils">
<dt>boml.boml_optimizer.BOMLOptimizer.aggregate_all(</dt>
<dd><p class="first last">aggregation_fn=None,
gradient_clip=None
)</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
</li>
<li><dl class="first docutils">
<dt>Args:</dt>
<dd><ul class="first last simple">
<li>aggregation_fn:Optional operation to aggregate multiple outer_gradients (for the same meta parameter),by (default: reduce_mean)</li>
<li>gradient_clip: optional operation to clip the aggregated outer gradients</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Returns: None</p>
</li>
</ul>
</li>
</ol>
<blockquote>
<div>Finally, aggregate_all has to be called to aggregate gradient of different tasks, and define operations to apply outer gradients and update meta parametes.</div></blockquote>
<ol class="arabic" start="6">
<li><p class="first">run:</p>
<ul>
<li><dl class="first docutils">
<dt>Aliases:</dt>
<dd><ul class="first simple">
<li>boml.boml_optimizer.BOMLOptimizer.run()</li>
</ul>
<div class="last highlight-sh"><div class="highlight"><pre><span></span>boml.boml_optimizer.BOMLOptimizer.run<span class="o">(</span>
        <span class="nv">inner_objective_feed_dicts</span><span class="o">=</span>None,
        <span class="nv">outer_objective_feed_dicts</span><span class="o">=</span>None,
        <span class="nv">session</span><span class="o">=</span>None,
        <span class="nv">_skip_hyper_ts</span><span class="o">=</span>False,
        <span class="nv">_only_hyper_ts</span><span class="o">=</span>False,
        <span class="nv">callback</span><span class="o">=</span>None
<span class="o">)</span>
</pre></div>
</div>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Args:</dt>
<dd><ul class="first last simple">
<li>inner_objective_feed_dicts: an optional feed dictionary for the inner problem. Can be a function of step, which accounts for, e.g. stochastic gradient descent.</li>
<li>outer_objective_feed_dicts: an optional feed dictionary for the outer optimization problem (passed to the evaluation of outer objective). Can be a function of hyper-iterations steps (i.e. global variable), which may account for, e.g. stochastic evaluation of outer objective.</li>
<li>session: optional session</li>
<li>callback: optional callback function of signature (step (int), feed_dictionary, <cite>tf.Session</cite>) -&gt; None that are called after every forward iteration.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Returns: None</p>
</li>
</ul>
</li>
</ol>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="modules.html" title="previous chapter">Core Modules</a></li>
      <li>Next: <a href="extension.html" title="next chapter">Extensible Modules</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/built_in.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Yaohua Liu, Risheng Liu.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="_sources/built_in.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>