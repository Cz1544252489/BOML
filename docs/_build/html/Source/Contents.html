
<!DOCTYPE html>

<html lang="Python">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Welcome to the documentation for PyBML &#8212; PyBML 1.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="welcome-to-the-documentation-for-pybml">
<h1>Welcome to the documentation for PyBML<a class="headerlink" href="#welcome-to-the-documentation-for-pybml" title="Permalink to this headline">¶</a></h1>
<div class="section" id="contents-div0-id-a0-div0">
<h2>Contents <div0 id="a0"></div0><a class="headerlink" href="#contents-div0-id-a0-div0" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="#a1">Introduction </a><br></p></li>
<li><p><a class="reference external" href="#a2">Installation and requirements </a></p></li>
<li><p><a class="reference external" href="#a3">Quickly build your bilevel meta-learning model </a></p>
<ul class="simple">
<li><p><a class="reference external" href="#a31">Core Modules </a></p></li>
<li><p><a class="reference external" href="#a32">Core Built-in functions of BMLHOptimizer </a></p></li>
<li><p><a class="reference external" href="#a33">Simple Training Example</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#a4">Modification and Extension </a></p></li>
<li><p><a class="reference external" href="#a5">Author and liscense</a></p></li>
</ol>
</div>
<div class="section" id="introduction-div1-id-a1-div1">
<h2>Introduction <div1 id="a1"></div1><a class="headerlink" href="#introduction-div1-id-a1-div1" title="Permalink to this headline">¶</a></h2>
<p>PyBML is a Bilevel Optimization Library in Python for Multi-Task and Meta Learning. Before reading the documentation, you could refer to <a class="reference external" href="https://github.com/liuyaohua918/pybml/edit/master/README">View on GitHub</a> for a brief introduction about meta learning and PyBML. <br>
Here we provide detailed instruction to quickly get down to your research, test performance of popular algorithms and new ideas.</p>
</div>
<div class="section" id="installation-and-requirements-div2-id-a2-div2">
<h2>Installation and requirements  <div2 id="a2"></div2><a class="headerlink" href="#installation-and-requirements-div2-id-a2-div2" title="Permalink to this headline">¶</a></h2>
<p>PyBML implements various meta learning algorithms based on <a class="reference external" href="https://www.tensorflow.org/install/pip">TensorFlow</a>, which is one of the most popular macheine learning platform. Besides, <a class="reference external" href="https://numpy.org/install/">Numpy</a> and basical image processing modules are required for  installation. <br>
We also provide <a class="reference external" href="https://github.com/liuyaohua918/pybml/requirements.txt">requirements.txt</a> as reference for version control.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Install from GitHub page：

git clone https://github.com/liuyaohua918/pybml.git

python setup.py install 

pip install requirements.txt

2. use pip instruction

pip install py_bml
</pre></div>
</div>
</div>
<div class="section" id="quickly-build-your-bilevel-meta-learning-model-div3-id-a3-div3">
<h2>Quickly build your bilevel meta-learning model <div3 id="a3"></div3><a class="headerlink" href="#quickly-build-your-bilevel-meta-learning-model-div3-id-a3-div3" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>Core Modules: <div3 id="a31"></div3></p>
<ol>
<li><p>Dataloader</p>
<ul class="simple">
<li><p>Related:</p>
<ul>
<li><p>pybml.Dataloader.meta_omniglot <br></p></li>
<li><p>pybml.Dataloader.meta_mini_imagenet <br></p></li>
<li><p>pybml.Dataloader.mnist <br></p></li>
</ul>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">meta_omniglot</span><span class="p">(</span>
    <span class="n">folder</span><span class="o">=</span><span class="n">DATA_FOLDER</span><span class="p">,</span> 
    <span class="n">std_num_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">examples_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">examples_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">one_hot_enc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">_rand</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> 
    <span class="n">n_splits</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">pybml</span><span class="o">.</span><span class="n">meta_mini_imagenet</span><span class="p">(</span>
  <span class="n">folder</span><span class="o">=</span><span class="n">DATA_FOLDER</span><span class="p">,</span> 
  <span class="n">sub_folders</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
  <span class="n">std_num_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">examples_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
  <span class="n">examples_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
  <span class="n">resize</span><span class="o">=</span><span class="mi">84</span><span class="p">,</span> 
  <span class="n">one_hot_enc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
  <span class="n">load_all_images</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
  <span class="n">h5</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
</pre></div>
</div>
<p>pybml.DataLoader manages different datasets and generate bathes of tasks for training and testing.</p>
<ul>
<li><p>Args：<br></p>
<ul class="simple">
<li><p>folder: root folder name. Use os module to modify the path to the datasets<br></p></li>
<li><p>std_num_classes: standard number of classes for N-way classification<br>
- examples_train:standard number of examples to be picked in each generated per classes for training (eg .1 shot, examples_train=1)<br></p></li>
<li><p>examples_test: standard number of examples to be picked in each generated per classes for testing</p></li>
<li><p>one_hot_enc: one hot encoding<br></p></li>
<li><p>_rand: random seed or RandomState for generate training, validation, testing meta-datasets split<br></p></li>
<li><p>n_splits: num classes per split<br></p></li>
</ul>
</li>
<li><p>Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">py_bml</span><span class="o">.</span><span class="n">meta_omniglot</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                        <span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">num_examples</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">examples_test</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>Returns: an initialized instance of data loader</p></li>
</ul>
</li>
<li><p>Experiment</p>
<ul class="simple">
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Dataloader.Experiment</p></li>
</ul>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Experiment</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>pybml.Experiment manages inputs, outputs and task-specific parameters.</p>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>dataset: initialized instance of Dataloader<br></p></li>
<li><p>dtype: default tf.float32<br></p></li>
</ul>
</li>
<li><p>Attributes:<br></p>
<ul>
<li><p>x: input placeholder of input for your defined lower level problem<br></p></li>
<li><p>y: label placeholder of output for yourdefined lower level problem<br></p></li>
<li><p>x_:input placeholder of input for your defined upper level problem<br></p></li>
<li><p>y_:label placeholder of output for your defined upper level problem<br></p></li>
<li><p>model: used to restore the task-specific model <br></p></li>
<li><p>errors: dictionary to restore defined loss functions of different levels<br></p></li>
<li><p>scores: dictionary to restore defined accuracies functions<br></p></li>
<li><p>optimizers: dictonary to restore optimized chosen for inner and outer loop optimization<br></p></li>
</ul>
</li>
<li><p>Usage:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ex</span> <span class="o">=</span> <span class="n">pybml</span><span class="o">.</span><span class="n">Experiment</span><span class="p">(</span><span class="n">datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">)</span>
<span class="n">ex</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="s1">&#39;training&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">py_bml</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">pred</span><span class="o">=</span><span class="n">ex</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">ex</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;HyperOptim&#39;</span><span class="p">)</span>
<span class="n">ex</span><span class="o">.</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">out</span><span class="p">),</span> <span class="mi">1</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">ex</span><span class="o">.</span><span class="n">optimizers</span><span class="p">[</span><span class="s1">&#39;apply_updates&#39;</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pybml</span><span class="o">.</span><span class="n">BMLOptSGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">lr0</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">ex</span><span class="o">.</span><span class="n">errors</span><span class="p">[</span><span class="s1">&#39;training&#39;</span><span class="p">],</span><span class="n">var_list</span><span class="o">=</span><span class="n">ex</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">var_list</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Returns: an initialized instance of Experiment</p></li>
</ul>
</li>
<li><p>BMLHOptimizer</p>
<ul class="simple">
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Core.BMLHOptimizer</p></li>
</ul>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="p">(</span>
    <span class="n">Method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">inner_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">outer_method</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">truncate_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">experiments</span><span class="o">=</span><span class="p">[]</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>BMLHOptimizer is the main class in <code class="docutils literal notranslate"><span class="pre">pybml</span></code>, which takes responsibility for the whole process of model construnction and back propagation.</p>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>Method: define basic method for following training process, it should be included in [<code class="docutils literal notranslate"><span class="pre">HyperOptim</span></code>, <code class="docutils literal notranslate"><span class="pre">BilevelOptim</span></code>], <code class="docutils literal notranslate"><span class="pre">HyperOptim</span></code> type includes methods like <code class="docutils literal notranslate"><span class="pre">MAML</span></code>, <code class="docutils literal notranslate"><span class="pre">FOMAML</span></code>, <code class="docutils literal notranslate"><span class="pre">TNet</span></code>, <code class="docutils literal notranslate"><span class="pre">WarpGrad</span></code>; <code class="docutils literal notranslate"><span class="pre">BilevelOptim</span></code> type includes methods like <code class="docutils literal notranslate"><span class="pre">BDA</span></code>, <code class="docutils literal notranslate"><span class="pre">RHG</span></code>, <code class="docutils literal notranslate"><span class="pre">TRHG</span></code>, <code class="docutils literal notranslate"><span class="pre">Implicit</span> <span class="pre">HG</span></code>, <code class="docutils literal notranslate"><span class="pre">DARTS</span></code>;<br></p></li>
<li><p>inner_method: method chosen for solving LLproblem, including [<code class="docutils literal notranslate"><span class="pre">Trad</span></code> ,<code class="docutils literal notranslate"><span class="pre">Simple</span></code>, <code class="docutils literal notranslate"><span class="pre">Aggr</span></code>], <code class="docutils literal notranslate"><span class="pre">BilevelOptim</span></code> type choose either <code class="docutils literal notranslate"><span class="pre">Trad</span></code> for traditional optimization strategies or <code class="docutils literal notranslate"><span class="pre">Aggr</span></code> for Gradient Aggragation optimization 'HyperOptim' type should choose <code class="docutils literal notranslate"><span class="pre">Simple</span></code>, and set specific parameters for detailed method choices like FOMAML or TNet.<br></p></li>
<li><p>outer_method: method chosen for solving LLproblem, including [<code class="docutils literal notranslate"><span class="pre">Reverse</span></code> ,<code class="docutils literal notranslate"><span class="pre">Simple</span></code>, <code class="docutils literal notranslate"><span class="pre">Forward</span></code>, <code class="docutils literal notranslate"><span class="pre">Implcit</span></code>], <code class="docutils literal notranslate"><span class="pre">HyperOptim</span></code> type should choose Simple, and set specific parameters for detailed method choices like <code class="docutils literal notranslate"><span class="pre">FOMAML</span></code></p></li>
<li><p>truncate_iter: specific parameter for <code class="docutils literal notranslate"><span class="pre">Truncated</span> <span class="pre">RHG</span></code> method, defining number of iterations to truncate in the Back propagation process<br></p></li>
<li><p>experiments: list of experiment objects that has already been initialized <br></p></li>
</ul>
</li>
<li><p>Usage:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ex</span> <span class="o">=</span> <span class="n">pybml</span><span class="o">.</span><span class="n">Experiment</span><span class="p">(</span><span class="n">pybml</span><span class="o">.</span><span class="n">meta_omniglot</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">pybml_ho</span> <span class="o">=</span> <span class="n">pybml</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="p">(</span>
    <span class="n">Method</span><span class="o">=</span><span class="s1">&#39;HyperOptim&#39;</span><span class="p">,</span> 
    <span class="n">inner_method</span><span class="o">=</span><span class="s1">&#39;Simple&#39;</span><span class="p">,</span> 
    <span class="n">outer_method</span><span class="o">=</span><span class="s1">&#39;Simple&#39;</span><span class="p">,</span>
    <span class="n">experiments</span><span class="o">=</span><span class="n">ex</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Utility functions:</p>
<ul>
<li><p>learning_rate(): returns defined inner learning rate</p></li>
<li><p>meta_learning_rate(): returns defined outer learning rate</p></li>
<li><p>Method: return defined method type</p></li>
<li><p>param_dict: return the dictionary that restores general parameters, like use_T,use_Warp, output shape of defined model, learn_lr, s, t, alpha, first_order.</p></li>
</ul>
</li>
<li><p>Returns: an initialized instance of BMLHOptimizer</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Core Built-in functions of BMLHOptimizer: <div3 id="a32"></div3></p>
<ol>
<li><p>BMLHOptimizer.Meta_model:</p>
<ul class="simple">
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Core.BMLHOptimizer.Meta_model()</p></li>
</ul>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Core</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="o">.</span><span class="n">Meta_model</span><span class="p">(</span>
    <span class="n">_input</span><span class="p">,</span> 
    <span class="n">dataset</span><span class="p">,</span> 
    <span class="n">meta_model</span><span class="o">=</span><span class="s1">&#39;v1&#39;</span><span class="p">,</span> 
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Hyper_Net&#39;</span><span class="p">,</span> 
    <span class="n">use_T</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">use_Warp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">model_args</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This method must be called once at first to build meta modules and initialize meta parameters and neural networks.</p>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>_input: orginal input for neural network construction;</p></li>
<li><p>dataset: which dataset to use for training and testing. It should be initialized before being passed into the function</p></li>
<li><p>meta_model: model chosen for neural network construction, <code class="docutils literal notranslate"><span class="pre">v1</span></code> for C4L with fully connected layer,<code class="docutils literal notranslate"><span class="pre">v2</span></code> for Residual blocks with fully connected layer.</p></li>
<li><p>name: name for Meta model modules used for BMLNet initialization</p></li>
<li><p>use_T: whether to use T layer for C4L neural networks</p></li>
</ul>
</li>
</ul>
</li>
<li><p>BMLHOptimizer.Base_model:</p>
<ul class="simple">
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Core.BMLHOptimizer.Base_model()</p></li>
</ul>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Core</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="o">.</span><span class="n">Base_model</span><span class="p">(</span>
    <span class="n">_input</span><span class="p">,</span> 
    <span class="n">meta_learner</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Task_Net&#39;</span><span class="p">,</span>
    <span class="n">weights_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This method has to be called for every experiment and takes responsibility for defining task-specific modules and inner optimizers.</p>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>_input: orginal input for neural network construction of task-specific module;</p></li>
<li><p>meta_learner: returned value of Meta_model function, which is a instance of BMLNet or its child classes</p></li>
<li><p>name: name for Base model modules used for BMLNet initialization</p></li>
<li><p>weights_initializer: initializer function for task_specific network, called by 'BilevelOptim' method</p></li>
</ul>
</li>
<li><p>Returns: task-specific model part</p></li>
</ul>
</li>
<li><p>BMLHOptimizer.LL_problem:</p>
<ul class="simple">
<li><p>Aliases:
- pybml.Core.BMLHOptimizer.LL_Problem()</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Core</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="o">.</span><span class="n">LL_Problem</span><span class="p">(</span>
      <span class="n">inner_objective</span><span class="p">,</span>
      <span class="n">learning_rate</span><span class="p">,</span> 
      <span class="n">T</span><span class="p">,</span> 
      <span class="n">inner_objective_optimizer</span><span class="o">=</span><span class="s1">&#39;SGD&#39;</span><span class="p">,</span> 
      <span class="n">outer_objective</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">learn_lr</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
      <span class="n">alpha_init</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
      <span class="n">s</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
      <span class="n">learn_alpha</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
      <span class="n">learn_st</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="n">learn_alpha_itr</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
      <span class="n">var_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
      <span class="n">init_dynamics_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
      <span class="n">first_order</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
      <span class="n">loss_func</span><span class="o">=</span><span class="n">utils</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">,</span> 
      <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
      <span class="n">beta1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
      <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
      <span class="n">regularization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
      <span class="n">experiment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
      <span class="n">scalor</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> 
      <span class="o">**</span><span class="n">inner_kargs</span>
<span class="p">)</span>
</pre></div>
</div>
<p>After construction of neural networks, solutions to lower level problems should be regulated in LL_Problem.</p>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>inner_objective: loss function for the inner optimization problem</p></li>
<li><p>learning_rate: step size for inner loop optimization</p></li>
<li><p>T: numbers of steps for inner gradient descent optimization</p></li>
<li><p>inner_objective_optimizer: Optimizer type for the outer parameters, should be in list [<code class="docutils literal notranslate"><span class="pre">SGD</span></code>,<code class="docutils literal notranslate"><span class="pre">Momentum</span></code>,<code class="docutils literal notranslate"><span class="pre">Adam</span></code>]</p></li>
<li><p>outer_objective: loss function for the outer optimization problem, which need to be claimed in BDA agorithm</p></li>
<li><p>alpha_init: initial value of ratio of inner objective to outer objective in BDA algorithm</p></li>
<li><p>s,t: coefficients of aggregation of inner and outer objectives in BDA algorithm, default to be 1.0</p></li>
<li><p>learn_alpha: specify parameter for BDA algorithm to decide whether to initialize alpha as a hyper parameter</p></li>
<li><p>learn_alpha_itr: parameter for BDA algorithm to specify whether to initialize alpha as a vector, of which every dimension's value is step-wise scale factor fot the optimization process</p></li>
<li><p>learn_st: specify parameter for BDA algorithm to decide whether to initialize s and t as hyper parameters</p></li>
<li><p>first_order: specific parameter to define whether to use implement first order MAML, default to be <code class="docutils literal notranslate"><span class="pre">FALSE</span></code></p></li>
<li><p>loss_func: specifying which type of loss function is used for the maml-based method, which should be consistent with the form to compute the inner objective</p></li>
<li><p>momentum: specific parameter for Optimizer.BMLOptMomentum to set initial value of momentum</p></li>
<li><p>beta1, beta2: specific parameter for Optimizer.BMLOptMomentum to set initial value of Adam</p></li>
<li><p>regularization: whether to add regularization terms in the inner objective</p></li>
<li><p>experiment: instance of Experiment to use in the Lower Level Problem, especifially needed in the <code class="docutils literal notranslate"><span class="pre">HyperOptim</span></code> type of method.</p></li>
<li><p>scalor: coefficient of regularization term in the objective function.</p></li>
<li><p>var_list: optional list of variables (of the inner optimization problem)from</p></li>
<li><p>init_dynamics_dict: optional dictrionary that defines Phi_0 (see <code class="docutils literal notranslate"><span class="pre">OptimizerDict.set_init_dynamics</span></code>)</p></li>
<li><p>inner_kargs: optional arguments to pass to <code class="docutils literal notranslate"><span class="pre">py_bml.core.optimizer.minimize</span></code></p></li>
</ul>
</li>
<li><p>Returns: task-specific model part</p></li>
</ul>
</li>
<li><p>BMLHOptimizer.UL_Problem</p>
<ul>
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Core.BMLHOptimizer.UL_Problem()</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Core</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="o">.</span><span class="n">UL_Problem</span><span class="p">(</span>
    <span class="n">outer_objective</span><span class="p">,</span> 
    <span class="n">meta_learning_rate</span><span class="p">,</span> 
    <span class="n">inner_grad</span><span class="p">,</span>
    <span class="n">meta_param</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">outer_objective_optimizer</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> 
    <span class="n">Reptile</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">Darts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> 
    <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
    <span class="n">global_step</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
<p>This method define upper level problems and choose optimizers to optimize meta parameters, which should be called afer LL_Problem.</p>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>outer_objective: scalar tensor for the outer objective</p></li>
<li><p>meta_learning_rate: step size for outer loop optimization</p></li>
<li><p>inner_grad: Returned value of py_bml.BMLHOptimizer.LLProblem()</p></li>
<li><p>meta_param: optional list of outer parameters and model parameters</p></li>
<li><p>outer_objective_optimizer: Optimizer type for the outer parameters, should be in list [<code class="docutils literal notranslate"><span class="pre">SGD</span></code>,<code class="docutils literal notranslate"><span class="pre">Momentum</span></code>,<code class="docutils literal notranslate"><span class="pre">Adam</span></code>]</p></li>
<li><p>Reptile: BOOLEAN, specific parameters to define whether to implement <code class="docutils literal notranslate"><span class="pre">Reptile</span></code> algorithm</p></li>
<li><p>Darts: BOOLEAN, specific parameters to define whether to implement 'DARTS' algorithm</p></li>
<li><p>epsilon: Float, cofffecients to be used in DARTS algorithm</p></li>
<li><p>momentum: specific parameters to be used to initialize <code class="docutils literal notranslate"><span class="pre">Momentum</span></code> algorithm</p></li>
<li><p>beta1, beta2: specific parameters to be used to initialize <code class="docutils literal notranslate"><span class="pre">Adam</span></code></p></li>
<li><p>global_step: optional global step. By default tries to use the last variable in the collection GLOBAL_STEP</p></li>
</ul>
</li>
<li><p>Returns：meta_param list, used for debugging</p></li>
</ul>
</li>
<li><p>Aggregate_all:</p>
<ul>
<li><p>Aliases:</p>
<ul class="simple">
<li><p>pybml.Core.BMLHOptimizer.Aggregate_all()</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Core</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="o">.</span><span class="n">Aggregate_all</span><span class="p">(</span>
    <span class="n">aggregation_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">gradient_clip</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Args:</p>
<ul class="simple">
<li><p>aggregation_fn:Optional operation to aggregate multiple outer_gradients (for the same meta parameter),by (default: reduce_mean)</p></li>
<li><p>gradient_clip: optional operation to clip the aggregated outer gradients</p></li>
</ul>
</li>
<li><p>Returns: None
Finally, Aggregate_all has to be called to aggregate gradient of different tasks, and define operations to apply outer gradients and update meta parametes.</p></li>
</ul>
</li>
<li><p>run:</p>
<ul class="simple">
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Core.BMLHOptimizer.run()</p></li>
</ul>
</li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pybml</span><span class="o">.</span><span class="n">Core</span><span class="o">.</span><span class="n">BMLHOptimizer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="n">inner_objective_feed_dicts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">outer_objective_feed_dicts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">train_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer_feed_dict</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">session</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
    <span class="n">online</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">_skip_hyper_ts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">_only_hyper_ts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">callback</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Args:</p>
<ul>
<li><p>inner_objective_feed_dicts: an optional feed dictionary for the inner problem. Can be a function of step, which accounts for, e.g. stochastic gradient descent.</p></li>
<li><p>outer_objective_feed_dicts: an optional feed dictionary for the outer optimization problem (passed to the evaluation of outer objective). Can be a function of hyper-iterations steps (i.e. global variable), which may account for, e.g. stochastic evaluation of outer objective.</p></li>
<li><p>train_batches: used for Reptile Algorithm, which needs to generates mini batches of images and labels during one training step</p></li>
<li><p>initializer_feed_dict:  an optional feed dictionary for the initialization of inner problems variables. Can be a function of hyper-iterations steps (i.e. global variable), which may account for, e.g. stochastic initialization.</p></li>
<li><p>session: optional session</p></li>
<li><p>online: default <code class="docutils literal notranslate"><span class="pre">False</span></code> if <code class="docutils literal notranslate"><span class="pre">True</span></code> performs the online version of the algorithms (i.e. does not reinitialize the state after at each run).</p></li>
<li><p>callback: optional callback function of signature (step (int), feed_dictionary, <code class="docutils literal notranslate"><span class="pre">tf.Session</span></code>) -&gt; None that are called after every forward iteration.</p></li>
</ul>
</li>
<li><p>Returns: None</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Simple Running Example <div3 id="a33"></div3></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    from py_bml import utils
    from py_bml.script_helper import *
    dataset = py_bml.meta_omniglot(args.num_classes, (args.examples_train, args.examples_test))
    ex = py_bml.BMLExperiment(dataset)
    # build network structure and define hyperparameters
    pybml_ho = py_bml.BMLHOptimizer(&#39;HyperOptim&#39;, &#39;Aggr&#39;, &#39;Reverse&#39;)
    meta_learner = pybml_ho.Meta_learner(ex.x, dataset, &#39;v1&#39;, args.use_T)
    ex.model = pybml_ho.Base_learner(meta_learner.out, meta_learner)
    # define Lower-level problems
    loss_inner = utils.cross_entropy_loss(ex.model.out, ex.y)
    inner_grad = pybml_ho.LL_problem(loss_inner, args.lr, args.T, experiment=ex)
    # define Upper-level problems
    loss_outer = utils.cross_entropy_loss(ex.model.re_forward(ex.x_).out, ex.y_)
    pybml_ho.UL_problem(loss_outer, args.mlr, inner_grad, hyper_list=py_bml.extension.hyperparameters())
    pybml_ho.Aggregate_all()
    # meta training step
    with utils.get_default_session():
        for itr in range(args.meta_train_iterations):
            train_batch = BatchQueueMock(dataset.train, 1,args.meta_batch_size，utils.get_rand_state())
            tr_fd, v_fd = feed_dicts(train_batch)
            pybml_ho.run(tr_fd, v_fd)
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="modification-and-extension-div4-id-a4-div4">
<h2>Modification and extension  <div4 id="a4"></div4><a class="headerlink" href="#modification-and-extension-div4-id-a4-div4" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Extensible Base Calsses and Modules</p>
<ol class="simple">
<li><p>BMLNet</p>
<ul>
<li><p>Aliases：</p>
<ul>
<li><p>pybml.Networks.BMLNet</p></li>
</ul>
</li>
<li><p>Methods to be overridden:</p>
<ul>
<li><p>forward()：
uses defined convolutional neural networks with initial input</p></li>
<li><p>re_forward(new_input):
reuses defined convolutional with new input and update the output results</p></li>
<li><p>create_outer_parameters():
this method creates parameters of upper level problems, and adds them to define collections called <code class="docutils literal notranslate"><span class="pre">METAPARAMETERS</span></code></p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>var_collections: collections to restore meta parameters created in the so called scope</p></li>
</ul>
</li>
<li><p>Returns: dictionary that indexes the outer parameters</p></li>
</ul>
</li>
<li><p>create_model_parameters():
this method creates model parameters of upper level problems like <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">layer</span></code> or <code class="docutils literal notranslate"><span class="pre">Warp</span> <span class="pre">Layer</span></code> , and adds them to define collections called <code class="docutils literal notranslate"><span class="pre">METAPARAMETERS</span></code></p></li>
</ul>
</li>
<li><p>Utility functions:</p>
<ul>
<li><p>get_conv_weight(bmlnet, layer, initializer):</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>bmlnet: initialized instance of BMLNet</p></li>
<li><p>layer: int32, the layer-th weight of convolutional block to be created</p></li>
<li><p>initializer: the tensorflow initializer used to initialize the filters
-Returns: created parameter</p></li>
</ul>
</li>
</ul>
</li>
<li><p>get_bias_weight(bmlnet, layer, initializer):</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>bmlnet: initialized instance of BMLNet</p></li>
<li><p>layer: int32, the layer-th bias of convolutional block to be created</p></li>
<li><p>initializer: the tensorflow initializer used to initialize the bias</p></li>
</ul>
</li>
<li><p>Returns: created parameter</p></li>
</ul>
</li>
<li><p>get_identity(dim, name, conv=True):</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>dim: the dimension of identity metrix</p></li>
<li><p>name: name to initialize the metrix</p></li>
<li><p>conv: BOOLEAN , whether initialize the metrix or initialize the real value, default to be True</p></li>
</ul>
</li>
<li><p>Returns: the created parameter</p></li>
</ul>
</li>
<li><p>conv_block(bmlnet, cweight, bweight):
uses defined convolutional weight and bias with current ouput of bmlnet</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>bmlnet: initialized instance of BMLNet</p></li>
<li><p>cweight: parameter of convolutional filter</p></li>
<li><p>bweight: parameter of bias for convolutional neural networks</p></li>
</ul>
</li>
</ul>
</li>
<li><p>conb_block_t(bmlnet, conv_weight, conv_bias, zweight):
uses defined convolutional weight, bias, and weights of t layer  with current ouput of bmlnet</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>bmlnet: initialized instance of BMLNet</p></li>
<li><p>cweight: parameter of convolutional filter</p></li>
<li><p>bweight: parameter of bias for convolutional neural networks</p></li>
</ul>
</li>
</ul>
</li>
<li><p>conv_block_warp(bmlnet, cweight, bweight, zweight, zbias):
uses defined convolutional weight, bias and filters of warp layer  with current ouput of bmlnet</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>bmlnet: initialized instance of BMLNet</p></li>
<li><p>cweight: parameter of convolutional filter</p></li>
<li><p>bweight: parameter of bias for convolutional neural networks</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>BMLInnerGrad</p>
<ul>
<li><p>Aliases:</p>
<ul>
<li><p>pybml.LLProblem.BMLInnerGrad</p></li>
</ul>
</li>
<li><p>Methods to be overridden:</p>
<ul>
<li><p>compute_gradients(bml_opt, loss_inner, loss_outer=None,inner_method=None, param_dict=OrderedDict(), var_list=None, **inner_kargs):
delivers equivalent functionality to the method called compute_gradients() in <code class="docutils literal notranslate"><span class="pre">tf.train.Optimizer</span></code></p></li>
<li><p>Args:</p>
<ul>
<li><p>bml_opt: instance of pybml.Optimizer.BMLOpt, which is automatically create by the method in <code class="docutils literal notranslate"><span class="pre">pybml.Core.BMLHOptimizer</span></code></p></li>
<li><p>loss_inner: inner objective, which could be passed by <code class="docutils literal notranslate"><span class="pre">pybml.Core.BMLHOptimizer.LL_Problem</span></code> or called directly.</p></li>
<li><p>loss_outer: outer objective,which could be passed automatically by <code class="docutils literal notranslate"><span class="pre">pybml.Core.BMLHOptimizer.LL_Problem</span></code>, or called directly</p></li>
<li><p>param_dict: automatically passed by 'pybml.Core.BMLHOptimizer.LL_Problem'</p></li>
<li><p>var_list: list of lower level variables</p></li>
<li><p>inner_kargs: optional arguments, which are same as <code class="docutils literal notranslate"><span class="pre">tf.train.Optimizer</span></code></p></li>
</ul>
</li>
<li><p>Returns：self</p></li>
</ul>
</li>
<li><p>Utility functions:</p>
<ul>
<li><p>apply_updates():
Descent step, as returned by <code class="docutils literal notranslate"><span class="pre">tf.train.Optimizer.apply_gradients</span></code>.</p></li>
<li><p>initialization():
a list of operations that return the values of the state variables for this learning dynamics after the execution of the initialization operation. If an initial dynamics is set, then it also executed.</p></li>
<li><p>state():
A generator for all the state variables (optimized variables and possibly auxiliary variables) being optimized</p></li>
</ul>
</li>
</ul>
</li>
<li><p>BMLOuterGrad</p>
<ul>
<li><p>Aliases:</p>
<ul>
<li><p>pybml.UL_Problem.BMLOuterGrad</p></li>
</ul>
</li>
<li><p>Methods to be overridden:</p>
<ul>
<li><p>compute_gradients(outer_objective, bml_inner_grad, meta_param=None):</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>bml_inner_grad: OptimzerDict object resulting from the inner objective optimization.</p></li>
<li><p>outer_objective: A loss function for the outer parameters (scalar tensor)</p></li>
<li><p>meta_param: Optional list of outer parameters to consider. If not provided will get all variables in the hyperparameter collection in the current scope.</p></li>
</ul>
</li>
<li><p>Returns: list of meta parameters involved in the computation</p></li>
</ul>
</li>
<li><p>apply_gradients( inner_objective_feed_dicts=None, outer_objective_feed_dicts=None, initializer_feed_dict=None, param_dict=OrderedDict(), train_batches=None, experiments= [], global_step=None, session=None, online=False, callback=None)</p>
<ul>
<li><p>Args:</p>
<ul>
<li><p>inner_objective_feed_dicts: Optional feed dictionary for the inner objective</p></li>
<li><p>outer_objective_feed_dicts: Optional feed dictionary for the outer objective
(note that this is not used in ForwardHG since hypergradients are not
variables)</p></li>
<li><p>initializer_feed_dict: Optional feed dictionary for the inner objective</p></li>
<li><p>global_step: Optional global step for the optimization process</p></li>
<li><p>param_dict: dictionary of parameters passed by <code class="docutils literal notranslate"><span class="pre">pybml.Core.BMLHOptimizer</span></code></p></li>
<li><p>train_batches: mini batches of data, needed when Reptile Algorithm are implemented</p></li>
<li><p>session: Optional session (otherwise will take the default session)</p></li>
<li><p>experiments: list of instances of <code class="docutils literal notranslate"><span class="pre">Experiment</span></code>, needed when Reptile Algorithm are implemented</p></li>
<li><p>online: Performs the computation of the outer gradient in the online (or &quot;real time&quot;) mode. Note that <code class="docutils literal notranslate"><span class="pre">ReverseHG</span></code> and <code class="docutils literal notranslate"><span class="pre">ForwardHG</span></code> behave differently.</p></li>
<li><p>callback: callback funciton for the forward optimization</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Utility functions:</p>
<ul>
<li><p>hgrads_hvars(hyper_list=None, aggregation_fn=None, process_fn=None):
Method for getting outergradient and outer parameters as required by apply_gradient methods from tensorflow optimizers.
- Args：
- meta_param: Optional list of outer parameters to consider. If not provided will get all variables in the hyperparameter collection in the current scope.
- aggregation_fn: Optional operation to aggregate multiple hypergradients (for the same hyperparameter),
by default reduce_mean
- process_fn: Optional operation like clipping to be applied.</p></li>
<li><p>initialization():
Returns groups of operation that initializes the variables in the computational graph</p></li>
<li><p>state():
returns current state values of lower level variables</p></li>
</ul>
</li>
</ul>
</li>
<li><p>BMLOpt</p>
<ul>
<li><p>Aliases:</p>
<ul>
<li><p>pybml.Optimizer.BMLOpt</p></li>
</ul>
</li>
<li><p>Methods to be overridden:</p>
<ul>
<li><p>minimize(loss_inner, var_list=None, global_step=None, gate_gradients=tf.train.Optimizer.GATE_OP,
aggregation_method=None, colocate_gradients_with_ops=False, name=None, grad_loss=None):</p>
<ul>
<li><p>Returns: an <code class="docutils literal notranslate"><span class="pre">bml_inner_grad</span></code> object relative to this minimization, same as <code class="docutils literal notranslate"><span class="pre">tf.train.Optimizer.minimize.</span></code></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Utility functions:</p>
<ul>
<li><p>learning_rate():
- Returns: the step size of this optimizer</p></li>
</ul>
</li>
<li><p>Utility Functions</p>
<ul>
<li><p>get_dafault_session():
get and return the default tensorflow session</p></li>
</ul>
</li>
<li><p>BatchQueueMock:
generates batches of taskes and feed them into corresponding placeholders.</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Utility Modules:</p>
<ul>
<li><p>get_default_session():
gets and returns the default tensorflow session</p></li>
<li><p>BatchQueueMock():
responsible for generates batches of taskes and feed them into corresponding placeholders.</p></li>
<li><p>cross_entropy_loss(pred, label, method):
return loss function that matches different methods in [<code class="docutils literal notranslate"><span class="pre">BilevelOptim</span></code>,<code class="docutils literal notranslate"><span class="pre">HyperOptim</span></code>]</p></li>
<li><p>vectorize_all(var_list, name=None):
Vectorize the variables in the list named var_list with the given name</p></li>
<li><p>remove_from_collectinon(key,*var_list):
removes the variables in the var_list according to the given Graph key</p></li>
<li><p>set_gpu():
set primary parameters of GPU configuration.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="authors-and-license-div5-id-a5-div5">
<h2>Authors and license<div5 id="a5"></div5><a class="headerlink" href="#authors-and-license-div5-id-a5-div5" title="Permalink to this headline">¶</a></h2>
<p>MIT License</p>
<p>Copyright (c) 2020 Yaohua Liu</p>
<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>
<p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PyBML</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, BMLS.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/Source/Contents.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>